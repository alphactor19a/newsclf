{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa480b31-79b9-4051-b79d-4087091fe336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 01:15:45.348377: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-09 01:15:45.355639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749446145.363999   37147 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749446145.366560   37147 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749446145.373343   37147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749446145.373352   37147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749446145.373353   37147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749446145.373354   37147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-09 01:15:45.375860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/matthias/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification,AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "models = [\n",
    "    'microsoft/deberta-v3-xsmall',\n",
    "    'microsoft/deberta-v3-small',\n",
    "    'microsoft/deberta-v3-large',\n",
    "    'microsoft/deberta-v3-base',\n",
    "]\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "device = 'cuda'\n",
    "attn_implementation = 'eager'# 'sdpa' #('flash_attention_2' if device in {'cuda', 'auto'} else 'sdpa')\n",
    "torch_dtype = (torch.bfloat16 if device in {'cuda', 'auto'} else torch.float16)\n",
    "torch_dtype = torch.bfloat16\n",
    "torch_dtype = torch.float32\n",
    "\n",
    "model_id = 'microsoft/deberta-v3-base'\n",
    "model_id = 'microsoft/deberta-v3-small'\n",
    "model_id = 'microsoft/deberta-v3-xsmall'\n",
    "#model_id = 'distilbert-base-uncased'\n",
    "#deberta_clf = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "deberta = AutoModel.from_pretrained(model_id, \n",
    "                                   attn_implementation=attn_implementation,\n",
    "                                   torch_dtype=torch_dtype,\n",
    "                                   #num_labels=3, \n",
    "                                   )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a306f27-afe6-46ec-8b26-0e359f466de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hello world.[SEP]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(['Hello world.'])['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86865523-b59e-42bf-b418-d2d6f8237335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([ 'Hello world.', 'A news article.'], truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d304ba0-1940-4753-8ebc-755739223ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import logit\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "def _class_probabilities(cumulative_probabilities):\n",
    "    P = cumulative_probabilities\n",
    "    K = P.shape[-1]+1\n",
    "    result = []\n",
    "    for k in range(K):\n",
    "        if k == 0:\n",
    "            result.append( P[:,k].unsqueeze(1) )\n",
    "        elif k < K-1:\n",
    "            result.append( (P[:,k] - P[:,k-1]).unsqueeze(1) )\n",
    "        else:\n",
    "            result.append( (1 - P[:,k-1]).unsqueeze(1) )\n",
    "    \n",
    "    result = torch.cat(result, dim=-1)\n",
    "    return result\n",
    "\n",
    "def _predict_class(cumulative_probabilities):\n",
    "    class_probabilities = _class_probabilities(cumulative_probabilities)\n",
    "    return class_probabilities.argmax(dim=-1)\n",
    "\n",
    "# define ordinal classification head\n",
    "class OrdinalRegressionHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes, link_function=nn.Sigmoid(), \n",
    "                 dtype=torch_dtype, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(hidden_dim, 1, bias=True)\n",
    "        \n",
    "        thresh_init = torch.tensor([0]+[1]*(num_classes-2), dtype=torch.float32)\n",
    "        self.raw_thresholds = nn.Parameter(thresh_init, requires_grad=True)\n",
    "        self.link_function = link_function\n",
    "\n",
    "        if isinstance(link_function, nn.Sigmoid):\n",
    "            self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.loss_func = nn.BCELoss()\n",
    "        #self = self.to(torch_dtype)\n",
    "        self.device = device\n",
    "        self = self.to(device)\n",
    "        \n",
    "    @property\n",
    "    def theta(self):\n",
    "        return torch.cumsum(self.raw_thresholds**2, dim=0)\n",
    "    \n",
    "    def forward(self, x, targets=None, verbose=False):\n",
    "        # x is the [CLS] hidden states\n",
    "        # upcast to float32 generally\n",
    "        logits = self.linear(x.to(self.raw_thresholds.dtype)).squeeze(-1)  # shape: [batch]\n",
    "        thresholds = self.theta \n",
    "        #thresholds = torch.cumsum(self.raw_thresholds**2, dim=0)\n",
    "        #thresholds = self.raw_thresholds\n",
    "        #print(thresholds)\n",
    "        logits = logits.unsqueeze(1)#.repeat(1, thresholds.size(0))\n",
    "        thresholds = thresholds.unsqueeze(0).repeat(logits.size(0), 1)\n",
    "        \n",
    "        #print('logits_shape', logits.shape)\n",
    "        #print('thresholds_shape', thresholds.shape)\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        threshold_logits = thresholds - logits\n",
    "        probs = self.link_function(threshold_logits)\n",
    "        \n",
    "        if targets is not None:\n",
    "            #print(targets, type(targets))\n",
    "            if not isinstance(targets, torch.Tensor):\n",
    "                targets = torch.LongTensor(targets)\n",
    "            \n",
    "            targets = targets.to(x.device).unsqueeze(-1)\n",
    "            range_ = torch.arange(self.num_classes-1).unsqueeze(0).repeat_interleave(batch_size, 0).to(x.device)\n",
    "\n",
    "            #print(targets.shape, range_.shape)\n",
    "            bce_targets = (targets <= range_).to(x.dtype)\n",
    "            \n",
    "            #print(bce_targets)\n",
    "            if verbose:\n",
    "                print('targets', targets)\n",
    "                #print('range', range_)\n",
    "                print('bce_targets', bce_targets)\n",
    "                print('logits', logits)\n",
    "                print('cum_probs', probs)\n",
    "                print('class probabilities', _class_probabilities(probs))\n",
    "                print('theta', self.theta)\n",
    "                print(self.link_function, self.loss_func)\n",
    "            \n",
    "            if isinstance(self.link_function, nn.Sigmoid):\n",
    "                # use BCEWithLogitsLoss for numerical stability\n",
    "                loss = self.loss_func(threshold_logits, bce_targets)\n",
    "            else:\n",
    "                loss = self.loss_func(probs, bce_targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return threshold_logits, probs, loss, logits\n",
    "\n",
    "class PretrainedModelForOrdinalSequenceClassification(nn.Module):\n",
    "    def __init__(self, model, num_classes=3, link_function=nn.Sigmoid()):\n",
    "        super(PretrainedModelForOrdinalSequenceClassification, self).__init__()\n",
    "        self.device = model.device\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = model.config.hidden_size\n",
    "        self.clf_head = OrdinalRegressionHead(self.hidden_dim, \n",
    "                                              num_classes, \n",
    "                                              link_function=link_function,\n",
    "                                              dtype=torch_dtype,\n",
    "                                              device=self.model.device)\n",
    "        self.device = self.model.device\n",
    "    def gradient_checkpointing_enable(self, *args, **kwargs):\n",
    "        return self.model.gradient_checkpointing_enable(*args, **kwargs)\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        targets = labels\n",
    "        dev = self.model.device\n",
    "        outputs = self.model(input_ids=input_ids.to(dev), \n",
    "                             attention_mask=attention_mask.to(dev), \n",
    "                             **kwargs)\n",
    "        x = outputs.last_hidden_state[:,0,:] # [CLS] token embedding\n",
    "        #print(x.shape)\n",
    "        threshold_logits, probs, loss, logits = self.clf_head(x, targets=targets)\n",
    "        \n",
    "        clf_outputs = SequenceClassifierOutput(loss=loss, \n",
    "                                               logits=threshold_logits, \n",
    "                                               hidden_states=x, \n",
    "                                               attentions=outputs.attentions)\n",
    "        class_probabilities = _class_probabilities(probs)\n",
    "        class_predictions = _predict_class(probs)\n",
    "        clf_outputs.class_probabilities = class_probabilities\n",
    "        clf_outputs.predicted_class = class_predictions\n",
    "        clf_outputs.base_logits = logits\n",
    "        return clf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a76839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze embeddings\n",
    "deberta.embeddings.word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12ef4a4-3fb1-451e-89c2-689563166435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf = PretrainedModelForOrdinalSequenceClassification(deberta, num_classes=3)\n",
    "#model_clf = deberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce4c73-fa7f-440a-82ef-fe6a43734041",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051e5490-d095-4d49-b248-167d5c907e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle as pkl\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import re\n",
    "from tqdm import tqdm; tqdm.pandas()\n",
    "import os.path as op\n",
    "import os\n",
    "\n",
    "df_dataset = pd.read_csv('Dataset-framing_annotations-Llama-3.3-70B-Instruct-Turbo.csv')\n",
    "\n",
    "output_dir = f'model_training-OrdinalClassifier-{model_id.split(\"/\")[-1]}'\n",
    "\n",
    "# induce partitions\n",
    "try: os.makedirs(output_dir)\n",
    "except FileExistsError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60906472-22f2-4672-8bec-02107bf09b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_ = df_dataset[['concept', 'source', 'dateTimePub', 'FRAMING_CLASS']]\n",
    "\n",
    "test_size = .025\n",
    "seed = 125\n",
    "\n",
    "try:\n",
    "    with open(op.join(output_dir, 'train_test_part.pkl'), 'rb') as file:\n",
    "        partition_ids = pkl.load(file)\n",
    "    train, val = partition_ids['train'], partition_ids['validation']\n",
    "except FileNotFoundError:\n",
    "    train, val = train_test_split(np.array(range(len(df_dataset_))), test_size=test_size, random_state=seed)\n",
    "    train, val = train.squeeze(), val.squeeze()\n",
    "    with open(op.join(output_dir, 'train_test_part.pkl'), 'wb') as file:\n",
    "        pkl.dump({'train': train, 'validation': val}, file)\n",
    "\n",
    "def shorten_to_n_words(text, n=1500):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    if len(words) <= n:\n",
    "        return text  # no truncation needed\n",
    "    \n",
    "    # Find the index where the n-th word ends\n",
    "    count = 0\n",
    "    end_index = len(text)\n",
    "    for match in re.finditer(r'\\b\\w+\\b', text):\n",
    "        count += 1\n",
    "        if count == n:\n",
    "            end_index = match.end()\n",
    "            break\n",
    "    \n",
    "    return text[:end_index].rstrip() + \"[truncated]...\"\n",
    "\n",
    "def format_prompt_with_article(title, body, max_words=2000):\n",
    "    body = shorten_to_n_words(body, n=max_words)\n",
    "    article_input = f'Title: {title}[SEP]{body}'\n",
    "    return article_input\n",
    "\n",
    "def format_prompt_from_row(row, max_words=2000):\n",
    "    return format_prompt_with_article(row.title, row.body, max_words=max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6feaf217-ee28-4d18-a3ac-0efab0d523f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import NamedSplit, DatasetDict, load_from_disk\n",
    "\n",
    "try:\n",
    "    ds = load_from_disk(op.join(output_dir, 'train_val_dataset.ds'))\n",
    "except FileNotFoundError:\n",
    "    df_dataset_['text'] = [ format_prompt_from_row(row) for row in tqdm(df_dataset.iloc, total=len(df_dataset)) ]\n",
    "    \n",
    "    class_order = [ 'NEUTRAL', 'LOADED', 'ALARMIST' ]\n",
    "    df_dataset_['labels'] = df_dataset_.FRAMING_CLASS.progress_apply(lambda s: class_order.index(s.strip().upper()))\n",
    "        \n",
    "    ds_train = Dataset.from_pandas(df_dataset_.iloc[train], split=NamedSplit('train'))\n",
    "    ds_val = Dataset.from_pandas(df_dataset_.iloc[val], split=NamedSplit('validation'))\n",
    "    \n",
    "    #assert False\n",
    "    def get_max_length(dataset, tokenizer=tokenizer):\n",
    "        return max(len(tokenizer(example[\"text\"])[\"input_ids\"]) for example in tqdm(dataset))\n",
    "    \n",
    "    #max_length = max(get_max_length(ds_train), get_max_length(ds_val))\n",
    "    max_length = 1500\n",
    "    \n",
    "    print('max length of:', max_length)\n",
    "    \n",
    "    # Tokenize with static padding\n",
    "    def tokenize_row(example, max_length=max_length, padding='max_length'):\n",
    "        tok = tokenizer(example[\"text\"], padding=padding, truncation=True, max_length=max_length)\n",
    "        #print(tok['input_ids'])\n",
    "        #print(len(tok['input_ids'][0]), len(tokenizer(example['text'])['input_ids'][0]))\n",
    "        return tok\n",
    "    \n",
    "    tok_train = lambda ex: tokenize_row(ex, padding='longest')\n",
    "    tok_val = tok_train # lambda ex: tokenize_row(ex, padding='max_length')\n",
    "    \n",
    "    ds_train = ds_train.map(tok_train, batched=True, batch_size=1, num_proc=1)\n",
    "    ds_val = ds_val.map(tok_val, batched=True, batch_size=1, num_proc=1)\n",
    "    \n",
    "    ds = DatasetDict({'train': ds_train, 'val': ds_val})\n",
    "    \n",
    "    ds.save_to_disk(op.join(output_dir, 'train_val_dataset.ds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7efb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.decode(tok_train(ds_train[1])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c0f8db-18f7-4f0e-b693-40c4583f2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 2/121888 [00:00<01:05, 1862.89it/s]\n"
     ]
    }
   ],
   "source": [
    "ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "ds_train, ds_val = ds['train'], ds['val']\n",
    "\n",
    "#ds_train = ds['train']\n",
    "lens_ = []\n",
    "for ex in tqdm(ds_train):\n",
    "    l = len(ex['input_ids'])\n",
    "    lens_.append(l)\n",
    "    if l != 1500:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d338612-9ddc-434c-9a03-07f059c8573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7284a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from datasets import load_metric\n",
    "import evaluate\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 1\n",
    "eval_batch_size = 1\n",
    "gradient_accumulation_steps = 5\n",
    "\n",
    "save_steps = 2_500\n",
    "eval_steps = save_steps\n",
    "\n",
    "#eval_steps = 5\n",
    "\n",
    "Num_train_examples = len(ds_train)\n",
    "optim = \"paged_adamw_32bit\"\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0#.00001\n",
    "gradient_checkpointing = False\n",
    "warmup_steps = 1_000\n",
    "\n",
    "\n",
    "num_epochs = 25\n",
    "max_steps = int(Num_train_examples/(batch_size*gradient_accumulation_steps)*num_epochs)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    max_steps=max_steps,\n",
    "    #num_train_epochs=EPOCHS,\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps, \n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=weight_decay,\n",
    "    #optim=optim, \n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    ")\n",
    "\n",
    "\n",
    "class OrdinalTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        #labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        #logits = outputs[0][:, 0]\n",
    "        #loss = torch.nn.functional.mse_loss(logits, labels)\n",
    "        loss = outputs.loss\n",
    "        if num_items_in_batch is not None:\n",
    "            loss = loss / num_items_in_batch\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_per_class_metrics(preds, targets, num_classes=None):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, and F1 for each class.\n",
    "    \n",
    "    Args:\n",
    "        preds: np.ndarray of shape (N,), predicted class indices\n",
    "        targets: np.ndarray of shape (N,), ground-truth class indices\n",
    "        num_classes: int, total number of classes (optional if all classes are present in data)\n",
    "\n",
    "    Returns:\n",
    "        metrics: dict with precision, recall, and f1 arrays of shape (num_classes,)\n",
    "    \"\"\"\n",
    "    if num_classes is None:\n",
    "        num_classes = max(np.max(preds), np.max(targets)) + 1\n",
    "    \n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1 = np.zeros(num_classes)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        tp = np.sum((preds == cls) & (targets == cls))\n",
    "        fp = np.sum((preds == cls) & (targets != cls))\n",
    "        fn = np.sum((preds != cls) & (targets == cls))\n",
    "        #print(cls, tp, fp, fn)\n",
    "    \n",
    "        precision[cls] = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall[cls] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        if precision[cls] + recall[cls] > 0:\n",
    "            f1[cls] = 2 * precision[cls] * recall[cls] / (precision[cls] + recall[cls])\n",
    "        else:\n",
    "            f1[cls] = 0.0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "#def _predict_class(logits):\n",
    "#    return logits.argmax(dim=-1)\n",
    "\n",
    "def compute_metrics(eval_pred, num_classes=3):\n",
    "    logits, labels = eval_pred\n",
    "    #print(eval_pred)\n",
    "    #print('logits', logits)\n",
    "    #print('labels', labels)\n",
    "    #print(logits, labels)\n",
    "\n",
    "    #print(logits[0].shape, logits[1].shape)\n",
    "    logits = logits[0]\n",
    "    #predictions = np.argmax(logits, axis=-1)\n",
    "    with torch.no_grad():\n",
    "        predictions = _predict_class(torch.sigmoid(torch.tensor(logits))).detach().cpu().numpy()\n",
    "    \n",
    "    result = metric.compute(predictions=predictions, references=labels) # dict with 'accuracy'\n",
    "    # partition the labels by targets and measure accuracy for each to ensure balance\n",
    "    per_class_metrics = compute_per_class_metrics(predictions, labels, num_classes=num_classes)\n",
    "    for cls in range(num_classes):\n",
    "        for metric_name in [ 'precision', 'recall', 'f1' ]:\n",
    "            metric_label = f'class{cls}_{metric_name}'\n",
    "            result[metric_label] = per_class_metrics[metric_name][cls]\n",
    "    \n",
    "    return result\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class EvaluateAtStepOneCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step == 1:\n",
    "            control.should_evaluate = True\n",
    "        return control\n",
    "\n",
    "#model_clf.model.enable_input_requires_grad()\n",
    "from torch.optim import AdamW, Adam, SGD\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW([ p for p in model_clf.parameters() if p.requires_grad ], \n",
    "                  lr=learning_rate, weight_decay=weight_decay, )\n",
    "#optimizer = SGD([ model_clf.clf_head.raw_thresholds, ], lr=1, weight_decay=0.)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, max_steps, -1).step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e638f1-5f4d-4c02-bf10-684462550b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OrdinalTrainer(\n",
    "    model=model_clf,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val, #.select(range(250)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EvaluateAtStepOneCallback()],\n",
    "    optimizers=(optimizer, scheduler), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3731c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pre = model_clf.model.embeddings.word_embeddings.weight.detach().cpu().to(torch.float32).numpy()\n",
    "pre_pre_l = model_clf.model.encoder.layer[1].attention.self.query_proj.weight.detach().cpu().to(torch.float32).numpy()\n",
    "pre_pre_t = model_clf.clf_head.raw_thresholds.detach().cpu().to(torch.float32).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf8c3bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0213,  0.0244, -0.3035,  ..., -0.0818,  0.0801, -0.2236],\n",
       "        [ 0.3567, -0.0761, -0.0009,  ...,  0.1059, -0.0573,  0.0448],\n",
       "        [-0.1027, -0.0442,  0.0193,  ...,  0.0282,  0.2102,  0.1320],\n",
       "        ...,\n",
       "        [-0.0829, -0.0483, -0.0480,  ...,  0.0344,  0.0036, -0.0932],\n",
       "        [ 0.0728,  0.1213, -0.1473,  ..., -0.0896, -0.0208, -0.1312],\n",
       "        [ 0.1752,  0.1418,  0.1919,  ...,  0.0189,  0.1738,  0.1304]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf.model.encoder.layer[1].attention.self.query_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56238f0d-7db4-47e2-8f73-ff76c26015b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_RESUME'] = 'must'\n",
    "os.environ['WANDB_RUN_ID'] = 'qmt1qeqy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4852d6f-f531-4ea6-b36d-6cadcd169907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthias-lalisse\u001b[0m (\u001b[33mmatthias-lalisse-inet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthias/projects/NewsSentiment/wandb/run-20250609_011553-qmt1qeqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">model_training-OrdinalClassifier-deberta-v3-xsmall</a></strong> to <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='484772' max='609440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [484772/609440 21:51:35 < 22:45:13, 1.52 it/s, Epoch 19.89/26]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Class0 Precision</th>\n",
       "      <th>Class0 Recall</th>\n",
       "      <th>Class0 F1</th>\n",
       "      <th>Class1 Precision</th>\n",
       "      <th>Class1 Recall</th>\n",
       "      <th>Class1 F1</th>\n",
       "      <th>Class2 Precision</th>\n",
       "      <th>Class2 Recall</th>\n",
       "      <th>Class2 F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>367500</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.334678</td>\n",
       "      <td>0.882598</td>\n",
       "      <td>0.940666</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.931900</td>\n",
       "      <td>0.790805</td>\n",
       "      <td>0.802800</td>\n",
       "      <td>0.796757</td>\n",
       "      <td>0.661202</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.711765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370000</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.342348</td>\n",
       "      <td>0.881958</td>\n",
       "      <td>0.938609</td>\n",
       "      <td>0.926610</td>\n",
       "      <td>0.932571</td>\n",
       "      <td>0.795082</td>\n",
       "      <td>0.792299</td>\n",
       "      <td>0.793688</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.703488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372500</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.341849</td>\n",
       "      <td>0.883237</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.925189</td>\n",
       "      <td>0.933142</td>\n",
       "      <td>0.791234</td>\n",
       "      <td>0.800467</td>\n",
       "      <td>0.795824</td>\n",
       "      <td>0.661202</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.711765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375000</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.333483</td>\n",
       "      <td>0.890595</td>\n",
       "      <td>0.936049</td>\n",
       "      <td>0.935606</td>\n",
       "      <td>0.935828</td>\n",
       "      <td>0.809133</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.807715</td>\n",
       "      <td>0.726708</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.735849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377500</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.327757</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.936019</td>\n",
       "      <td>0.935133</td>\n",
       "      <td>0.935576</td>\n",
       "      <td>0.811986</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.809133</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.751553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380000</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.303721</td>\n",
       "      <td>0.885797</td>\n",
       "      <td>0.946751</td>\n",
       "      <td>0.917614</td>\n",
       "      <td>0.931955</td>\n",
       "      <td>0.778867</td>\n",
       "      <td>0.834306</td>\n",
       "      <td>0.805634</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.729560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382500</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.373887</td>\n",
       "      <td>0.889635</td>\n",
       "      <td>0.921983</td>\n",
       "      <td>0.951231</td>\n",
       "      <td>0.936378</td>\n",
       "      <td>0.837596</td>\n",
       "      <td>0.764294</td>\n",
       "      <td>0.799268</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.726708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385000</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.335923</td>\n",
       "      <td>0.884837</td>\n",
       "      <td>0.940556</td>\n",
       "      <td>0.928977</td>\n",
       "      <td>0.934731</td>\n",
       "      <td>0.797897</td>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.797431</td>\n",
       "      <td>0.657609</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387500</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.309190</td>\n",
       "      <td>0.886756</td>\n",
       "      <td>0.945933</td>\n",
       "      <td>0.919508</td>\n",
       "      <td>0.932533</td>\n",
       "      <td>0.783991</td>\n",
       "      <td>0.834306</td>\n",
       "      <td>0.808366</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.723270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390000</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.312557</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.947522</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.793722</td>\n",
       "      <td>0.826138</td>\n",
       "      <td>0.809605</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.726727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392500</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.330832</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.933145</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.935788</td>\n",
       "      <td>0.812128</td>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.804476</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.716981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395000</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.331645</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.934025</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.936231</td>\n",
       "      <td>0.807118</td>\n",
       "      <td>0.820303</td>\n",
       "      <td>0.813657</td>\n",
       "      <td>0.766917</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.703448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397500</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.292528</td>\n",
       "      <td>0.889315</td>\n",
       "      <td>0.947471</td>\n",
       "      <td>0.922348</td>\n",
       "      <td>0.934741</td>\n",
       "      <td>0.783315</td>\n",
       "      <td>0.843641</td>\n",
       "      <td>0.812360</td>\n",
       "      <td>0.741497</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.717105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400000</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.310527</td>\n",
       "      <td>0.886756</td>\n",
       "      <td>0.945119</td>\n",
       "      <td>0.921402</td>\n",
       "      <td>0.933110</td>\n",
       "      <td>0.787305</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.805698</td>\n",
       "      <td>0.704142</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.730061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402500</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.306143</td>\n",
       "      <td>0.887716</td>\n",
       "      <td>0.944256</td>\n",
       "      <td>0.922348</td>\n",
       "      <td>0.933174</td>\n",
       "      <td>0.787778</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>0.807057</td>\n",
       "      <td>0.723926</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405000</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.299369</td>\n",
       "      <td>0.879079</td>\n",
       "      <td>0.958607</td>\n",
       "      <td>0.899148</td>\n",
       "      <td>0.927926</td>\n",
       "      <td>0.752588</td>\n",
       "      <td>0.848308</td>\n",
       "      <td>0.797586</td>\n",
       "      <td>0.681564</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407500</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.310459</td>\n",
       "      <td>0.885477</td>\n",
       "      <td>0.942968</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>0.933270</td>\n",
       "      <td>0.785393</td>\n",
       "      <td>0.815636</td>\n",
       "      <td>0.800229</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.728395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.361384</td>\n",
       "      <td>0.886756</td>\n",
       "      <td>0.935071</td>\n",
       "      <td>0.934186</td>\n",
       "      <td>0.934628</td>\n",
       "      <td>0.809069</td>\n",
       "      <td>0.791132</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.679775</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.722388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412500</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.324542</td>\n",
       "      <td>0.880038</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>0.910985</td>\n",
       "      <td>0.932171</td>\n",
       "      <td>0.776418</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.794989</td>\n",
       "      <td>0.611374</td>\n",
       "      <td>0.821656</td>\n",
       "      <td>0.701087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415000</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.318837</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.944605</td>\n",
       "      <td>0.928504</td>\n",
       "      <td>0.936485</td>\n",
       "      <td>0.798635</td>\n",
       "      <td>0.819137</td>\n",
       "      <td>0.808756</td>\n",
       "      <td>0.695906</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.725610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417500</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.303917</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.946195</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.935090</td>\n",
       "      <td>0.792952</td>\n",
       "      <td>0.840140</td>\n",
       "      <td>0.815864</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.737179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420000</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.323525</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.945763</td>\n",
       "      <td>0.924716</td>\n",
       "      <td>0.935121</td>\n",
       "      <td>0.790233</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.810011</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.719243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422500</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.338813</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>0.931957</td>\n",
       "      <td>0.940341</td>\n",
       "      <td>0.936130</td>\n",
       "      <td>0.812941</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.809607</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.715232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425000</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.332624</td>\n",
       "      <td>0.890275</td>\n",
       "      <td>0.939408</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.935837</td>\n",
       "      <td>0.801833</td>\n",
       "      <td>0.816803</td>\n",
       "      <td>0.809249</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.726115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.310818</td>\n",
       "      <td>0.885477</td>\n",
       "      <td>0.954074</td>\n",
       "      <td>0.914773</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.775620</td>\n",
       "      <td>0.838973</td>\n",
       "      <td>0.806054</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.706949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.377272</td>\n",
       "      <td>0.886436</td>\n",
       "      <td>0.936261</td>\n",
       "      <td>0.938920</td>\n",
       "      <td>0.937589</td>\n",
       "      <td>0.815177</td>\n",
       "      <td>0.777130</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.638743</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.701149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432500</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.342882</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>0.937973</td>\n",
       "      <td>0.937086</td>\n",
       "      <td>0.811231</td>\n",
       "      <td>0.792299</td>\n",
       "      <td>0.801653</td>\n",
       "      <td>0.676301</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.709091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.329156</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.941261</td>\n",
       "      <td>0.933239</td>\n",
       "      <td>0.937233</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.816803</td>\n",
       "      <td>0.810654</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.302101</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.947547</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>0.935507</td>\n",
       "      <td>0.787419</td>\n",
       "      <td>0.847141</td>\n",
       "      <td>0.816189</td>\n",
       "      <td>0.765517</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.735099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440000</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.338191</td>\n",
       "      <td>0.892514</td>\n",
       "      <td>0.941345</td>\n",
       "      <td>0.934659</td>\n",
       "      <td>0.937990</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.811409</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.732308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442500</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.334132</td>\n",
       "      <td>0.894434</td>\n",
       "      <td>0.936291</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.937840</td>\n",
       "      <td>0.809908</td>\n",
       "      <td>0.820303</td>\n",
       "      <td>0.815072</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.736486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445000</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.368587</td>\n",
       "      <td>0.889635</td>\n",
       "      <td>0.934496</td>\n",
       "      <td>0.938920</td>\n",
       "      <td>0.936703</td>\n",
       "      <td>0.815348</td>\n",
       "      <td>0.793466</td>\n",
       "      <td>0.804258</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.721713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447500</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.345254</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.939771</td>\n",
       "      <td>0.930871</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.801156</td>\n",
       "      <td>0.808635</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.717791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450000</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.332742</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.945595</td>\n",
       "      <td>0.929924</td>\n",
       "      <td>0.937694</td>\n",
       "      <td>0.800231</td>\n",
       "      <td>0.808635</td>\n",
       "      <td>0.804411</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452500</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.326257</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.947776</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>0.801812</td>\n",
       "      <td>0.826138</td>\n",
       "      <td>0.813793</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.740964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455000</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.321124</td>\n",
       "      <td>0.890595</td>\n",
       "      <td>0.953500</td>\n",
       "      <td>0.922348</td>\n",
       "      <td>0.937665</td>\n",
       "      <td>0.792873</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.811396</td>\n",
       "      <td>0.670270</td>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.725146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457500</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.356936</td>\n",
       "      <td>0.884517</td>\n",
       "      <td>0.940727</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.936251</td>\n",
       "      <td>0.801663</td>\n",
       "      <td>0.787631</td>\n",
       "      <td>0.794585</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.699140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.352869</td>\n",
       "      <td>0.897313</td>\n",
       "      <td>0.932867</td>\n",
       "      <td>0.947443</td>\n",
       "      <td>0.940099</td>\n",
       "      <td>0.830732</td>\n",
       "      <td>0.807468</td>\n",
       "      <td>0.818935</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.713376</td>\n",
       "      <td>0.734426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462500</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.315905</td>\n",
       "      <td>0.892194</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.937143</td>\n",
       "      <td>0.800227</td>\n",
       "      <td>0.822637</td>\n",
       "      <td>0.811277</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.738854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465000</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.326596</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.943934</td>\n",
       "      <td>0.924716</td>\n",
       "      <td>0.934226</td>\n",
       "      <td>0.790828</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.807539</td>\n",
       "      <td>0.717791</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.731250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467500</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.351997</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.936643</td>\n",
       "      <td>0.937973</td>\n",
       "      <td>0.937308</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.815636</td>\n",
       "      <td>0.813737</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.723127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470000</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.351173</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.933616</td>\n",
       "      <td>0.938920</td>\n",
       "      <td>0.936261</td>\n",
       "      <td>0.814252</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.813777</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.732673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472500</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.358410</td>\n",
       "      <td>0.887396</td>\n",
       "      <td>0.933396</td>\n",
       "      <td>0.935606</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.806375</td>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.801643</td>\n",
       "      <td>0.709877</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.721003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475000</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.348186</td>\n",
       "      <td>0.885477</td>\n",
       "      <td>0.943314</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.932471</td>\n",
       "      <td>0.790487</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.802299</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.732143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477500</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.372619</td>\n",
       "      <td>0.890275</td>\n",
       "      <td>0.927180</td>\n",
       "      <td>0.946496</td>\n",
       "      <td>0.936739</td>\n",
       "      <td>0.821951</td>\n",
       "      <td>0.786464</td>\n",
       "      <td>0.803816</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.716612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480000</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.888676</td>\n",
       "      <td>0.956931</td>\n",
       "      <td>0.915246</td>\n",
       "      <td>0.935624</td>\n",
       "      <td>0.781014</td>\n",
       "      <td>0.844807</td>\n",
       "      <td>0.811659</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.720238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482500</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.340653</td>\n",
       "      <td>0.892514</td>\n",
       "      <td>0.941738</td>\n",
       "      <td>0.933712</td>\n",
       "      <td>0.937708</td>\n",
       "      <td>0.809965</td>\n",
       "      <td>0.815636</td>\n",
       "      <td>0.812791</td>\n",
       "      <td>0.704142</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.730061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = op.join(output_dir, 'checkpoint-365000')\n",
    "#ckpt = False\n",
    "trainer.train(resume_from_checkpoint=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7910b-b5e5-41c7-8809-1b0e561d4fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4b8a0-adca-4ac3-9ecd-d4c0ef382ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d797495-f147-455d-aae1-e520220297d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2432198-2af0-49a9-95e3-e6082e570d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthias-lalisse\u001b[0m (\u001b[33mmatthias-lalisse-inet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthias/projects/NewsSentiment/wandb/run-20250608_202230-qmt1qeqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">model_training-OrdinalClassifier-deberta-v3-xsmall</a></strong> to <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='365664' max='365664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [365664/365664 2:27:15, Epoch 15/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Class0 Precision</th>\n",
       "      <th>Class0 Recall</th>\n",
       "      <th>Class0 F1</th>\n",
       "      <th>Class1 Precision</th>\n",
       "      <th>Class1 Recall</th>\n",
       "      <th>Class1 F1</th>\n",
       "      <th>Class2 Precision</th>\n",
       "      <th>Class2 Recall</th>\n",
       "      <th>Class2 F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>355000</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.337473</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.937663</td>\n",
       "      <td>0.815511</td>\n",
       "      <td>0.809802</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.732308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357500</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.893794</td>\n",
       "      <td>0.938418</td>\n",
       "      <td>0.937973</td>\n",
       "      <td>0.938196</td>\n",
       "      <td>0.818396</td>\n",
       "      <td>0.809802</td>\n",
       "      <td>0.814076</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.734568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360000</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.332885</td>\n",
       "      <td>0.895074</td>\n",
       "      <td>0.938389</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937944</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>0.816803</td>\n",
       "      <td>0.817280</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.744479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362500</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.334167</td>\n",
       "      <td>0.894434</td>\n",
       "      <td>0.938834</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.938166</td>\n",
       "      <td>0.817116</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.815205</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.741433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365000</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.335120</td>\n",
       "      <td>0.893474</td>\n",
       "      <td>0.938805</td>\n",
       "      <td>0.937027</td>\n",
       "      <td>0.937915</td>\n",
       "      <td>0.815728</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.813341</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at model_training-OrdinalClassifier-deberta-v3-xsmall/checkpoint-107500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=365664, training_loss=0.001416962195738434, metrics={'train_runtime': 8837.6479, 'train_samples_per_second': 206.879, 'train_steps_per_second': 41.376, 'total_flos': 0.0, 'train_loss': 0.001416962195738434, 'epoch': 15.000369191388815})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = op.join(output_dir, 'checkpoint-352500')\n",
    "#ckpt = False\n",
    "trainer.train(resume_from_checkpoint=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcbc8d2-be9e-4485-819c-87d1518d436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8c8f5-e0d7-41b5-9ae3-ce19f684374b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdac6ef-6ec9-4e04-adc5-ac27429cdc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthias-lalisse\u001b[0m (\u001b[33mmatthias-lalisse-inet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthias/projects/NewsSentiment/wandb/run-20250607_225432-qmt1qeqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">model_training-OrdinalClassifier-deberta-v3-xsmall</a></strong> to <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354528' max='365664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354528/365664 21:21:57 < 2:01:59, 1.52 it/s, Epoch 14.54/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Class0 Precision</th>\n",
       "      <th>Class0 Recall</th>\n",
       "      <th>Class0 F1</th>\n",
       "      <th>Class1 Precision</th>\n",
       "      <th>Class1 Recall</th>\n",
       "      <th>Class1 F1</th>\n",
       "      <th>Class2 Precision</th>\n",
       "      <th>Class2 Recall</th>\n",
       "      <th>Class2 F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.269640</td>\n",
       "      <td>0.890275</td>\n",
       "      <td>0.947087</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>0.935283</td>\n",
       "      <td>0.796193</td>\n",
       "      <td>0.829638</td>\n",
       "      <td>0.812571</td>\n",
       "      <td>0.699422</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242500</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.267697</td>\n",
       "      <td>0.887076</td>\n",
       "      <td>0.953040</td>\n",
       "      <td>0.912879</td>\n",
       "      <td>0.932527</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.843641</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.701149</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.737160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245000</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.298816</td>\n",
       "      <td>0.889315</td>\n",
       "      <td>0.943296</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.936322</td>\n",
       "      <td>0.801843</td>\n",
       "      <td>0.812135</td>\n",
       "      <td>0.806957</td>\n",
       "      <td>0.683616</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.724551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247500</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.294807</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.813737</td>\n",
       "      <td>0.815636</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.735202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.290952</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.953639</td>\n",
       "      <td>0.905777</td>\n",
       "      <td>0.929092</td>\n",
       "      <td>0.766206</td>\n",
       "      <td>0.841307</td>\n",
       "      <td>0.802002</td>\n",
       "      <td>0.681564</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.726190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252500</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.318336</td>\n",
       "      <td>0.883557</td>\n",
       "      <td>0.942085</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.933078</td>\n",
       "      <td>0.790138</td>\n",
       "      <td>0.803967</td>\n",
       "      <td>0.796992</td>\n",
       "      <td>0.664835</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.713864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255000</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.284197</td>\n",
       "      <td>0.887076</td>\n",
       "      <td>0.948267</td>\n",
       "      <td>0.919981</td>\n",
       "      <td>0.933910</td>\n",
       "      <td>0.785635</td>\n",
       "      <td>0.829638</td>\n",
       "      <td>0.807037</td>\n",
       "      <td>0.691860</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.723404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257500</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.285775</td>\n",
       "      <td>0.896993</td>\n",
       "      <td>0.942721</td>\n",
       "      <td>0.935133</td>\n",
       "      <td>0.938911</td>\n",
       "      <td>0.813212</td>\n",
       "      <td>0.833139</td>\n",
       "      <td>0.823055</td>\n",
       "      <td>0.751634</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.278173</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.945789</td>\n",
       "      <td>0.925189</td>\n",
       "      <td>0.935376</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.810472</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.731861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262500</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.312375</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.939943</td>\n",
       "      <td>0.933712</td>\n",
       "      <td>0.936817</td>\n",
       "      <td>0.808009</td>\n",
       "      <td>0.800467</td>\n",
       "      <td>0.804220</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.720238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265000</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.282211</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.948705</td>\n",
       "      <td>0.919508</td>\n",
       "      <td>0.933878</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.842474</td>\n",
       "      <td>0.813063</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.744479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267500</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.298423</td>\n",
       "      <td>0.888676</td>\n",
       "      <td>0.945736</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.934866</td>\n",
       "      <td>0.792601</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.808462</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.727829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.280594</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.947982</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.935476</td>\n",
       "      <td>0.789071</td>\n",
       "      <td>0.842474</td>\n",
       "      <td>0.814898</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.739550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272500</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.310130</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.939667</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.938108</td>\n",
       "      <td>0.811254</td>\n",
       "      <td>0.807468</td>\n",
       "      <td>0.809357</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275000</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.284695</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.945140</td>\n",
       "      <td>0.929924</td>\n",
       "      <td>0.937470</td>\n",
       "      <td>0.801561</td>\n",
       "      <td>0.838973</td>\n",
       "      <td>0.819840</td>\n",
       "      <td>0.741722</td>\n",
       "      <td>0.713376</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277500</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.300224</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.939610</td>\n",
       "      <td>0.935606</td>\n",
       "      <td>0.937604</td>\n",
       "      <td>0.809851</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.817341</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.742671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.294525</td>\n",
       "      <td>0.890595</td>\n",
       "      <td>0.949268</td>\n",
       "      <td>0.921402</td>\n",
       "      <td>0.935127</td>\n",
       "      <td>0.790287</td>\n",
       "      <td>0.835473</td>\n",
       "      <td>0.812252</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.746177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282500</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.314708</td>\n",
       "      <td>0.890595</td>\n",
       "      <td>0.936402</td>\n",
       "      <td>0.934186</td>\n",
       "      <td>0.935293</td>\n",
       "      <td>0.808140</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.809552</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.734177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285000</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.350007</td>\n",
       "      <td>0.893154</td>\n",
       "      <td>0.926165</td>\n",
       "      <td>0.950284</td>\n",
       "      <td>0.938070</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.784131</td>\n",
       "      <td>0.809151</td>\n",
       "      <td>0.729032</td>\n",
       "      <td>0.719745</td>\n",
       "      <td>0.724359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287500</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.346363</td>\n",
       "      <td>0.890275</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.939180</td>\n",
       "      <td>0.825926</td>\n",
       "      <td>0.780630</td>\n",
       "      <td>0.802639</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.711370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.286418</td>\n",
       "      <td>0.895393</td>\n",
       "      <td>0.947394</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.938337</td>\n",
       "      <td>0.800446</td>\n",
       "      <td>0.837806</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.751592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292500</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.299227</td>\n",
       "      <td>0.887716</td>\n",
       "      <td>0.947445</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.934485</td>\n",
       "      <td>0.785872</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.807714</td>\n",
       "      <td>0.703030</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.720497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295000</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.293661</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.944578</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>0.936231</td>\n",
       "      <td>0.799097</td>\n",
       "      <td>0.826138</td>\n",
       "      <td>0.812392</td>\n",
       "      <td>0.721212</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297500</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.308126</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>0.942584</td>\n",
       "      <td>0.932765</td>\n",
       "      <td>0.937649</td>\n",
       "      <td>0.806039</td>\n",
       "      <td>0.809802</td>\n",
       "      <td>0.807916</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.728916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.332152</td>\n",
       "      <td>0.892194</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.938447</td>\n",
       "      <td>0.814858</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.810557</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.718266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302500</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.346264</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.929538</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>0.820913</td>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.808763</td>\n",
       "      <td>0.748344</td>\n",
       "      <td>0.719745</td>\n",
       "      <td>0.733766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305000</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.321275</td>\n",
       "      <td>0.894434</td>\n",
       "      <td>0.935849</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.937618</td>\n",
       "      <td>0.818075</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.815682</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.739550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307500</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.316912</td>\n",
       "      <td>0.884517</td>\n",
       "      <td>0.943853</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.933461</td>\n",
       "      <td>0.787982</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.799310</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.716418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310000</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.331934</td>\n",
       "      <td>0.890595</td>\n",
       "      <td>0.933993</td>\n",
       "      <td>0.937973</td>\n",
       "      <td>0.935979</td>\n",
       "      <td>0.816229</td>\n",
       "      <td>0.798133</td>\n",
       "      <td>0.807080</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.734568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312500</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.322578</td>\n",
       "      <td>0.887076</td>\n",
       "      <td>0.942968</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>0.933270</td>\n",
       "      <td>0.792090</td>\n",
       "      <td>0.817970</td>\n",
       "      <td>0.804822</td>\n",
       "      <td>0.703488</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.735562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315000</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.310023</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.949194</td>\n",
       "      <td>0.919981</td>\n",
       "      <td>0.934359</td>\n",
       "      <td>0.786108</td>\n",
       "      <td>0.831972</td>\n",
       "      <td>0.808390</td>\n",
       "      <td>0.703488</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.735562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317500</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.325826</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.941148</td>\n",
       "      <td>0.931345</td>\n",
       "      <td>0.936221</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.816803</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.720238</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.744615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.314495</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.946705</td>\n",
       "      <td>0.925189</td>\n",
       "      <td>0.935824</td>\n",
       "      <td>0.797312</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.813714</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.742331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322500</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.320956</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.945657</td>\n",
       "      <td>0.922822</td>\n",
       "      <td>0.934100</td>\n",
       "      <td>0.792411</td>\n",
       "      <td>0.828471</td>\n",
       "      <td>0.810040</td>\n",
       "      <td>0.710059</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.736196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325000</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.327866</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>0.942843</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.936099</td>\n",
       "      <td>0.803899</td>\n",
       "      <td>0.817970</td>\n",
       "      <td>0.810873</td>\n",
       "      <td>0.703488</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.735562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327500</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.337050</td>\n",
       "      <td>0.892194</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.935606</td>\n",
       "      <td>0.936937</td>\n",
       "      <td>0.813599</td>\n",
       "      <td>0.809802</td>\n",
       "      <td>0.811696</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.734568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330000</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.346393</td>\n",
       "      <td>0.893474</td>\n",
       "      <td>0.934998</td>\n",
       "      <td>0.939867</td>\n",
       "      <td>0.937426</td>\n",
       "      <td>0.820452</td>\n",
       "      <td>0.805134</td>\n",
       "      <td>0.812721</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.739812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332500</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.324769</td>\n",
       "      <td>0.896033</td>\n",
       "      <td>0.942243</td>\n",
       "      <td>0.934659</td>\n",
       "      <td>0.938436</td>\n",
       "      <td>0.814007</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>0.820602</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.744479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335000</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.892514</td>\n",
       "      <td>0.939077</td>\n",
       "      <td>0.934186</td>\n",
       "      <td>0.936625</td>\n",
       "      <td>0.811628</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.813046</td>\n",
       "      <td>0.721212</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337500</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.319867</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.942078</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.936920</td>\n",
       "      <td>0.806636</td>\n",
       "      <td>0.822637</td>\n",
       "      <td>0.814558</td>\n",
       "      <td>0.723926</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.323511</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.940499</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>0.934223</td>\n",
       "      <td>0.798396</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.805780</td>\n",
       "      <td>0.710059</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.736196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342500</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.322586</td>\n",
       "      <td>0.892194</td>\n",
       "      <td>0.936493</td>\n",
       "      <td>0.935606</td>\n",
       "      <td>0.936049</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.746835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.321465</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.941487</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.935430</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.824971</td>\n",
       "      <td>0.813111</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.746835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347500</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.343432</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.935697</td>\n",
       "      <td>0.937027</td>\n",
       "      <td>0.936361</td>\n",
       "      <td>0.813459</td>\n",
       "      <td>0.803967</td>\n",
       "      <td>0.808685</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.741433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350000</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.327792</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.940727</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.936251</td>\n",
       "      <td>0.805075</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.809745</td>\n",
       "      <td>0.724551</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.746914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352500</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.342077</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.937205</td>\n",
       "      <td>0.939867</td>\n",
       "      <td>0.938534</td>\n",
       "      <td>0.822064</td>\n",
       "      <td>0.808635</td>\n",
       "      <td>0.815294</td>\n",
       "      <td>0.721212</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = op.join(output_dir, 'checkpoint-237500')\n",
    "#ckpt = False\n",
    "trainer.train(resume_from_checkpoint=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e632a-5a5f-4d09-8187-0a048a9690d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52221e0-e749-44e9-ba54-5f4c31ed71e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60bd34-0cbb-4ea7-adf7-556a651e4bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c94d5b-a9b8-4115-bccf-9c3fce9f780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_training-OrdinalClassifier-deberta-v3-xsmall'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45d303-0002-463d-a862-1f2abd943623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b61e7a-f45c-4c81-adf8-1942e93cc522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c9ec7-2fd7-446e-a457-19032de061ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthias-lalisse\u001b[0m (\u001b[33mmatthias-lalisse-inet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthias/projects/NewsSentiment/wandb/run-20250607_180052-qmt1qeqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">model_training-OrdinalClassifier-deberta-v3-xsmall</a></strong> to <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237704' max='365664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237704/365664 4:18:12 < 24:15:22, 1.47 it/s, Epoch 9.75/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Class0 Precision</th>\n",
       "      <th>Class0 Recall</th>\n",
       "      <th>Class0 F1</th>\n",
       "      <th>Class1 Precision</th>\n",
       "      <th>Class1 Recall</th>\n",
       "      <th>Class1 F1</th>\n",
       "      <th>Class2 Precision</th>\n",
       "      <th>Class2 Recall</th>\n",
       "      <th>Class2 F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>217500</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.298002</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.933712</td>\n",
       "      <td>0.933712</td>\n",
       "      <td>0.933712</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.805134</td>\n",
       "      <td>0.806075</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.740506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.281716</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.943133</td>\n",
       "      <td>0.926610</td>\n",
       "      <td>0.934798</td>\n",
       "      <td>0.797714</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.806005</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.726727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222500</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.279663</td>\n",
       "      <td>0.880678</td>\n",
       "      <td>0.962379</td>\n",
       "      <td>0.896307</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.751788</td>\n",
       "      <td>0.858810</td>\n",
       "      <td>0.801743</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.735905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.328868</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.937663</td>\n",
       "      <td>0.811098</td>\n",
       "      <td>0.801634</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.680233</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.711246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227500</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.276041</td>\n",
       "      <td>0.889315</td>\n",
       "      <td>0.945278</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.934642</td>\n",
       "      <td>0.790233</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.810011</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.731861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.274709</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.944848</td>\n",
       "      <td>0.924716</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.826138</td>\n",
       "      <td>0.808681</td>\n",
       "      <td>0.715152</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.732919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232500</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.302787</td>\n",
       "      <td>0.885797</td>\n",
       "      <td>0.947164</td>\n",
       "      <td>0.925189</td>\n",
       "      <td>0.936048</td>\n",
       "      <td>0.790857</td>\n",
       "      <td>0.807468</td>\n",
       "      <td>0.799076</td>\n",
       "      <td>0.654255</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.713043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235000</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.299710</td>\n",
       "      <td>0.896033</td>\n",
       "      <td>0.936941</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.939816</td>\n",
       "      <td>0.827751</td>\n",
       "      <td>0.807468</td>\n",
       "      <td>0.817484</td>\n",
       "      <td>0.715152</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.732919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237500</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.249365</td>\n",
       "      <td>0.896353</td>\n",
       "      <td>0.953431</td>\n",
       "      <td>0.920928</td>\n",
       "      <td>0.936898</td>\n",
       "      <td>0.794181</td>\n",
       "      <td>0.859977</td>\n",
       "      <td>0.825770</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = op.join(output_dir, 'checkpoint-215000')\n",
    "#ckpt = False\n",
    "trainer.train(resume_from_checkpoint=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a79ad0-4fcf-4ab4-b9b7-5c1f172c9f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccb8a0-85bc-47ca-9a35-063539335db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada1d5f-d5ce-484b-b2b1-c5249dbc134f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d925ae-340e-4dbd-8075-2b139cef2dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthias-lalisse\u001b[0m (\u001b[33mmatthias-lalisse-inet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthias/projects/NewsSentiment/wandb/run-20250606_015012-qmt1qeqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">model_training-OrdinalClassifier-deberta-v3-xsmall</a></strong> to <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy' target=\"_blank\">https://wandb.ai/matthias-lalisse-inet/huggingface/runs/qmt1qeqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215040' max='365664' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215040/365664 39:25:00 < 27:36:34, 1.52 it/s, Epoch 8.82/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Class0 Precision</th>\n",
       "      <th>Class0 Recall</th>\n",
       "      <th>Class0 F1</th>\n",
       "      <th>Class1 Precision</th>\n",
       "      <th>Class1 Recall</th>\n",
       "      <th>Class1 F1</th>\n",
       "      <th>Class2 Precision</th>\n",
       "      <th>Class2 Recall</th>\n",
       "      <th>Class2 F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.265326</td>\n",
       "      <td>0.703135</td>\n",
       "      <td>0.827669</td>\n",
       "      <td>0.980114</td>\n",
       "      <td>0.897464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.815287</td>\n",
       "      <td>0.327366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.273856</td>\n",
       "      <td>0.710173</td>\n",
       "      <td>0.739558</td>\n",
       "      <td>0.997633</td>\n",
       "      <td>0.849426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407942</td>\n",
       "      <td>0.719745</td>\n",
       "      <td>0.520737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.275872</td>\n",
       "      <td>0.698656</td>\n",
       "      <td>0.853383</td>\n",
       "      <td>0.967330</td>\n",
       "      <td>0.906791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192623</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.317210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.256707</td>\n",
       "      <td>0.708253</td>\n",
       "      <td>0.811719</td>\n",
       "      <td>0.983902</td>\n",
       "      <td>0.889555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240283</td>\n",
       "      <td>0.866242</td>\n",
       "      <td>0.376210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>0.297255</td>\n",
       "      <td>0.714331</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.994318</td>\n",
       "      <td>0.861185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.245900</td>\n",
       "      <td>0.249019</td>\n",
       "      <td>0.706334</td>\n",
       "      <td>0.855546</td>\n",
       "      <td>0.978693</td>\n",
       "      <td>0.912986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198592</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.325260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>0.245539</td>\n",
       "      <td>0.715931</td>\n",
       "      <td>0.811146</td>\n",
       "      <td>0.992424</td>\n",
       "      <td>0.892675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261993</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.406295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.257899</td>\n",
       "      <td>0.716891</td>\n",
       "      <td>0.796893</td>\n",
       "      <td>0.995739</td>\n",
       "      <td>0.885287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283368</td>\n",
       "      <td>0.878981</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.250116</td>\n",
       "      <td>0.713372</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.898065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241497</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.381208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.248025</td>\n",
       "      <td>0.709213</td>\n",
       "      <td>0.851457</td>\n",
       "      <td>0.982481</td>\n",
       "      <td>0.912288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206096</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.335697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.251502</td>\n",
       "      <td>0.717210</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.998106</td>\n",
       "      <td>0.871074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336683</td>\n",
       "      <td>0.853503</td>\n",
       "      <td>0.482883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.241225</td>\n",
       "      <td>0.713692</td>\n",
       "      <td>0.794318</td>\n",
       "      <td>0.992898</td>\n",
       "      <td>0.882576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275720</td>\n",
       "      <td>0.853503</td>\n",
       "      <td>0.416796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.240677</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.856493</td>\n",
       "      <td>0.980587</td>\n",
       "      <td>0.914349</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.051342</td>\n",
       "      <td>0.096810</td>\n",
       "      <td>0.211890</td>\n",
       "      <td>0.885350</td>\n",
       "      <td>0.341943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.232124</td>\n",
       "      <td>0.779910</td>\n",
       "      <td>0.812138</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.894233</td>\n",
       "      <td>0.945205</td>\n",
       "      <td>0.241540</td>\n",
       "      <td>0.384758</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.828025</td>\n",
       "      <td>0.545073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.232498</td>\n",
       "      <td>0.809021</td>\n",
       "      <td>0.840982</td>\n",
       "      <td>0.989110</td>\n",
       "      <td>0.909051</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.360560</td>\n",
       "      <td>0.519328</td>\n",
       "      <td>0.423948</td>\n",
       "      <td>0.834395</td>\n",
       "      <td>0.562232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.247450</td>\n",
       "      <td>0.852527</td>\n",
       "      <td>0.853708</td>\n",
       "      <td>0.991951</td>\n",
       "      <td>0.917652</td>\n",
       "      <td>0.917836</td>\n",
       "      <td>0.534422</td>\n",
       "      <td>0.675516</td>\n",
       "      <td>0.647399</td>\n",
       "      <td>0.713376</td>\n",
       "      <td>0.678788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.204700</td>\n",
       "      <td>0.223840</td>\n",
       "      <td>0.855726</td>\n",
       "      <td>0.894485</td>\n",
       "      <td>0.975379</td>\n",
       "      <td>0.933182</td>\n",
       "      <td>0.884268</td>\n",
       "      <td>0.570595</td>\n",
       "      <td>0.693617</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.802548</td>\n",
       "      <td>0.590164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>0.248486</td>\n",
       "      <td>0.846449</td>\n",
       "      <td>0.886695</td>\n",
       "      <td>0.978220</td>\n",
       "      <td>0.930212</td>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.518086</td>\n",
       "      <td>0.659243</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.866242</td>\n",
       "      <td>0.587473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.230386</td>\n",
       "      <td>0.873640</td>\n",
       "      <td>0.885972</td>\n",
       "      <td>0.974905</td>\n",
       "      <td>0.928314</td>\n",
       "      <td>0.863914</td>\n",
       "      <td>0.659277</td>\n",
       "      <td>0.747849</td>\n",
       "      <td>0.722973</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.701639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.237265</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.926818</td>\n",
       "      <td>0.947443</td>\n",
       "      <td>0.937017</td>\n",
       "      <td>0.832891</td>\n",
       "      <td>0.732789</td>\n",
       "      <td>0.779640</td>\n",
       "      <td>0.596244</td>\n",
       "      <td>0.808917</td>\n",
       "      <td>0.686486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.272536</td>\n",
       "      <td>0.878439</td>\n",
       "      <td>0.898994</td>\n",
       "      <td>0.973485</td>\n",
       "      <td>0.934758</td>\n",
       "      <td>0.886293</td>\n",
       "      <td>0.663944</td>\n",
       "      <td>0.759173</td>\n",
       "      <td>0.614213</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.683616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.220679</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.932093</td>\n",
       "      <td>0.948864</td>\n",
       "      <td>0.940404</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.784131</td>\n",
       "      <td>0.805273</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.713376</td>\n",
       "      <td>0.697819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.270923</td>\n",
       "      <td>0.885157</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.875549</td>\n",
       "      <td>0.697783</td>\n",
       "      <td>0.776623</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>0.709459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.226642</td>\n",
       "      <td>0.887396</td>\n",
       "      <td>0.935711</td>\n",
       "      <td>0.944129</td>\n",
       "      <td>0.939901</td>\n",
       "      <td>0.831004</td>\n",
       "      <td>0.763127</td>\n",
       "      <td>0.795620</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.802548</td>\n",
       "      <td>0.690411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.255283</td>\n",
       "      <td>0.861164</td>\n",
       "      <td>0.955067</td>\n",
       "      <td>0.905777</td>\n",
       "      <td>0.929769</td>\n",
       "      <td>0.764916</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.756342</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.878981</td>\n",
       "      <td>0.624434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.254625</td>\n",
       "      <td>0.887396</td>\n",
       "      <td>0.901267</td>\n",
       "      <td>0.976799</td>\n",
       "      <td>0.937514</td>\n",
       "      <td>0.880466</td>\n",
       "      <td>0.704784</td>\n",
       "      <td>0.782890</td>\n",
       "      <td>0.708609</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.694805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.240388</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.912171</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.939610</td>\n",
       "      <td>0.861930</td>\n",
       "      <td>0.750292</td>\n",
       "      <td>0.802246</td>\n",
       "      <td>0.744526</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.225258</td>\n",
       "      <td>0.892514</td>\n",
       "      <td>0.930427</td>\n",
       "      <td>0.949811</td>\n",
       "      <td>0.940019</td>\n",
       "      <td>0.827206</td>\n",
       "      <td>0.787631</td>\n",
       "      <td>0.806934</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.700965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.217332</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.935879</td>\n",
       "      <td>0.939867</td>\n",
       "      <td>0.937869</td>\n",
       "      <td>0.811475</td>\n",
       "      <td>0.808635</td>\n",
       "      <td>0.810053</td>\n",
       "      <td>0.728477</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.167900</td>\n",
       "      <td>0.246251</td>\n",
       "      <td>0.893794</td>\n",
       "      <td>0.921640</td>\n",
       "      <td>0.957860</td>\n",
       "      <td>0.939401</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.777130</td>\n",
       "      <td>0.807762</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>0.709459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.231121</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.938679</td>\n",
       "      <td>0.942235</td>\n",
       "      <td>0.940454</td>\n",
       "      <td>0.802013</td>\n",
       "      <td>0.836639</td>\n",
       "      <td>0.818961</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.573248</td>\n",
       "      <td>0.669145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.255866</td>\n",
       "      <td>0.893154</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.939860</td>\n",
       "      <td>0.848134</td>\n",
       "      <td>0.768961</td>\n",
       "      <td>0.806610</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.713415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.251370</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.932125</td>\n",
       "      <td>0.949337</td>\n",
       "      <td>0.940652</td>\n",
       "      <td>0.833962</td>\n",
       "      <td>0.773629</td>\n",
       "      <td>0.802663</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.676558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.252714</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.940980</td>\n",
       "      <td>0.936080</td>\n",
       "      <td>0.938524</td>\n",
       "      <td>0.813981</td>\n",
       "      <td>0.801634</td>\n",
       "      <td>0.807760</td>\n",
       "      <td>0.679558</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.727811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.235923</td>\n",
       "      <td>0.871401</td>\n",
       "      <td>0.961929</td>\n",
       "      <td>0.897254</td>\n",
       "      <td>0.928466</td>\n",
       "      <td>0.752432</td>\n",
       "      <td>0.812135</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.685567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.259396</td>\n",
       "      <td>0.895074</td>\n",
       "      <td>0.926706</td>\n",
       "      <td>0.957860</td>\n",
       "      <td>0.942026</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.773629</td>\n",
       "      <td>0.810018</td>\n",
       "      <td>0.687117</td>\n",
       "      <td>0.713376</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.889635</td>\n",
       "      <td>0.945113</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.937217</td>\n",
       "      <td>0.793953</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>0.810286</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.696486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.251688</td>\n",
       "      <td>0.895393</td>\n",
       "      <td>0.930491</td>\n",
       "      <td>0.950758</td>\n",
       "      <td>0.940515</td>\n",
       "      <td>0.837871</td>\n",
       "      <td>0.789965</td>\n",
       "      <td>0.813213</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.719243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.234731</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.933707</td>\n",
       "      <td>0.946970</td>\n",
       "      <td>0.940291</td>\n",
       "      <td>0.821302</td>\n",
       "      <td>0.809802</td>\n",
       "      <td>0.815511</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.695946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.253951</td>\n",
       "      <td>0.884197</td>\n",
       "      <td>0.954410</td>\n",
       "      <td>0.911932</td>\n",
       "      <td>0.932688</td>\n",
       "      <td>0.779605</td>\n",
       "      <td>0.829638</td>\n",
       "      <td>0.803844</td>\n",
       "      <td>0.647959</td>\n",
       "      <td>0.808917</td>\n",
       "      <td>0.719547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.281091</td>\n",
       "      <td>0.895713</td>\n",
       "      <td>0.920218</td>\n",
       "      <td>0.961174</td>\n",
       "      <td>0.940250</td>\n",
       "      <td>0.850192</td>\n",
       "      <td>0.774796</td>\n",
       "      <td>0.810745</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.716216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.256009</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.936270</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.941121</td>\n",
       "      <td>0.831502</td>\n",
       "      <td>0.794632</td>\n",
       "      <td>0.812649</td>\n",
       "      <td>0.682081</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.715152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>0.897313</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.951705</td>\n",
       "      <td>0.941011</td>\n",
       "      <td>0.831138</td>\n",
       "      <td>0.809802</td>\n",
       "      <td>0.820331</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.701389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.267158</td>\n",
       "      <td>0.895074</td>\n",
       "      <td>0.921256</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.939429</td>\n",
       "      <td>0.846056</td>\n",
       "      <td>0.775963</td>\n",
       "      <td>0.809495</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.726667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.277553</td>\n",
       "      <td>0.889635</td>\n",
       "      <td>0.919252</td>\n",
       "      <td>0.954072</td>\n",
       "      <td>0.936338</td>\n",
       "      <td>0.829360</td>\n",
       "      <td>0.771295</td>\n",
       "      <td>0.799274</td>\n",
       "      <td>0.766423</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.251650</td>\n",
       "      <td>0.889315</td>\n",
       "      <td>0.942953</td>\n",
       "      <td>0.931345</td>\n",
       "      <td>0.937113</td>\n",
       "      <td>0.802995</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.808116</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.705167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.138400</td>\n",
       "      <td>0.236625</td>\n",
       "      <td>0.888676</td>\n",
       "      <td>0.946984</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.934261</td>\n",
       "      <td>0.788779</td>\n",
       "      <td>0.836639</td>\n",
       "      <td>0.812005</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.716981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.262946</td>\n",
       "      <td>0.897313</td>\n",
       "      <td>0.920814</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.941694</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.779463</td>\n",
       "      <td>0.814138</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.245428</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.948267</td>\n",
       "      <td>0.919981</td>\n",
       "      <td>0.933910</td>\n",
       "      <td>0.779787</td>\n",
       "      <td>0.855309</td>\n",
       "      <td>0.815804</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.721088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.248282</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.951731</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.937785</td>\n",
       "      <td>0.795556</td>\n",
       "      <td>0.835473</td>\n",
       "      <td>0.815026</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.722892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.250053</td>\n",
       "      <td>0.896993</td>\n",
       "      <td>0.928704</td>\n",
       "      <td>0.949811</td>\n",
       "      <td>0.939139</td>\n",
       "      <td>0.830529</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.818236</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.735395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.235673</td>\n",
       "      <td>0.892194</td>\n",
       "      <td>0.949805</td>\n",
       "      <td>0.922822</td>\n",
       "      <td>0.936119</td>\n",
       "      <td>0.793407</td>\n",
       "      <td>0.842474</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.735202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.883237</td>\n",
       "      <td>0.943405</td>\n",
       "      <td>0.931345</td>\n",
       "      <td>0.937336</td>\n",
       "      <td>0.815822</td>\n",
       "      <td>0.770128</td>\n",
       "      <td>0.792317</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.853503</td>\n",
       "      <td>0.688946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.286684</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.926287</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.936051</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.777130</td>\n",
       "      <td>0.800962</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.719745</td>\n",
       "      <td>0.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.243983</td>\n",
       "      <td>0.895074</td>\n",
       "      <td>0.938834</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.938166</td>\n",
       "      <td>0.815972</td>\n",
       "      <td>0.822637</td>\n",
       "      <td>0.819291</td>\n",
       "      <td>0.738562</td>\n",
       "      <td>0.719745</td>\n",
       "      <td>0.729032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.231753</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.941543</td>\n",
       "      <td>0.930398</td>\n",
       "      <td>0.935937</td>\n",
       "      <td>0.795580</td>\n",
       "      <td>0.840140</td>\n",
       "      <td>0.817253</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.707904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.249125</td>\n",
       "      <td>0.873321</td>\n",
       "      <td>0.958248</td>\n",
       "      <td>0.891098</td>\n",
       "      <td>0.923454</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.845974</td>\n",
       "      <td>0.791485</td>\n",
       "      <td>0.657754</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.715116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.246625</td>\n",
       "      <td>0.894754</td>\n",
       "      <td>0.940256</td>\n",
       "      <td>0.938920</td>\n",
       "      <td>0.939588</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.852975</td>\n",
       "      <td>0.823662</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.274742</td>\n",
       "      <td>0.896353</td>\n",
       "      <td>0.936530</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.939844</td>\n",
       "      <td>0.831528</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.818720</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.732308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.245298</td>\n",
       "      <td>0.893154</td>\n",
       "      <td>0.941456</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.938998</td>\n",
       "      <td>0.812354</td>\n",
       "      <td>0.813302</td>\n",
       "      <td>0.812828</td>\n",
       "      <td>0.700599</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.283236</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.929335</td>\n",
       "      <td>0.946496</td>\n",
       "      <td>0.937837</td>\n",
       "      <td>0.833537</td>\n",
       "      <td>0.794632</td>\n",
       "      <td>0.813620</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.286935</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>0.936553</td>\n",
       "      <td>0.938553</td>\n",
       "      <td>0.813899</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.810082</td>\n",
       "      <td>0.678161</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.712991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.234039</td>\n",
       "      <td>0.888676</td>\n",
       "      <td>0.955423</td>\n",
       "      <td>0.913352</td>\n",
       "      <td>0.933914</td>\n",
       "      <td>0.774262</td>\n",
       "      <td>0.856476</td>\n",
       "      <td>0.813296</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.727848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.258270</td>\n",
       "      <td>0.894434</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.928504</td>\n",
       "      <td>0.937380</td>\n",
       "      <td>0.798024</td>\n",
       "      <td>0.848308</td>\n",
       "      <td>0.822398</td>\n",
       "      <td>0.755245</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.271966</td>\n",
       "      <td>0.887396</td>\n",
       "      <td>0.940019</td>\n",
       "      <td>0.927557</td>\n",
       "      <td>0.933746</td>\n",
       "      <td>0.795222</td>\n",
       "      <td>0.815636</td>\n",
       "      <td>0.805300</td>\n",
       "      <td>0.711656</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.266372</td>\n",
       "      <td>0.891235</td>\n",
       "      <td>0.947241</td>\n",
       "      <td>0.926610</td>\n",
       "      <td>0.936812</td>\n",
       "      <td>0.801130</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>0.814007</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.722892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.889635</td>\n",
       "      <td>0.948705</td>\n",
       "      <td>0.919508</td>\n",
       "      <td>0.933878</td>\n",
       "      <td>0.787813</td>\n",
       "      <td>0.844807</td>\n",
       "      <td>0.815315</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.725552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.264296</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.932420</td>\n",
       "      <td>0.934186</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.806265</td>\n",
       "      <td>0.810968</td>\n",
       "      <td>0.808610</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.727869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.321492</td>\n",
       "      <td>0.883237</td>\n",
       "      <td>0.934087</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.936733</td>\n",
       "      <td>0.823604</td>\n",
       "      <td>0.757293</td>\n",
       "      <td>0.789058</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.815287</td>\n",
       "      <td>0.690027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.286826</td>\n",
       "      <td>0.895074</td>\n",
       "      <td>0.937353</td>\n",
       "      <td>0.942235</td>\n",
       "      <td>0.939787</td>\n",
       "      <td>0.824582</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.815339</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.726708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177500</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.260428</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>0.944471</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.935214</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>0.830805</td>\n",
       "      <td>0.814180</td>\n",
       "      <td>0.717791</td>\n",
       "      <td>0.745223</td>\n",
       "      <td>0.731250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.256425</td>\n",
       "      <td>0.893154</td>\n",
       "      <td>0.945684</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.934356</td>\n",
       "      <td>0.793028</td>\n",
       "      <td>0.849475</td>\n",
       "      <td>0.820282</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.752475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182500</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.280095</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.940585</td>\n",
       "      <td>0.929451</td>\n",
       "      <td>0.934985</td>\n",
       "      <td>0.799324</td>\n",
       "      <td>0.827305</td>\n",
       "      <td>0.813073</td>\n",
       "      <td>0.756579</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.744337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.281824</td>\n",
       "      <td>0.887076</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.925189</td>\n",
       "      <td>0.933142</td>\n",
       "      <td>0.792986</td>\n",
       "      <td>0.817970</td>\n",
       "      <td>0.805284</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.730650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187500</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.251635</td>\n",
       "      <td>0.896993</td>\n",
       "      <td>0.943062</td>\n",
       "      <td>0.933239</td>\n",
       "      <td>0.938125</td>\n",
       "      <td>0.803769</td>\n",
       "      <td>0.845974</td>\n",
       "      <td>0.824332</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.742268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.235998</td>\n",
       "      <td>0.896033</td>\n",
       "      <td>0.951362</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.938580</td>\n",
       "      <td>0.792473</td>\n",
       "      <td>0.859977</td>\n",
       "      <td>0.824846</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192500</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.273858</td>\n",
       "      <td>0.894434</td>\n",
       "      <td>0.939971</td>\n",
       "      <td>0.934186</td>\n",
       "      <td>0.937070</td>\n",
       "      <td>0.811995</td>\n",
       "      <td>0.821470</td>\n",
       "      <td>0.816705</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.750789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.273632</td>\n",
       "      <td>0.875560</td>\n",
       "      <td>0.958690</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>0.928972</td>\n",
       "      <td>0.756124</td>\n",
       "      <td>0.828471</td>\n",
       "      <td>0.790646</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.690808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197500</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.301047</td>\n",
       "      <td>0.893794</td>\n",
       "      <td>0.932553</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.937603</td>\n",
       "      <td>0.824308</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.811611</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.744479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.275252</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.939351</td>\n",
       "      <td>0.931345</td>\n",
       "      <td>0.935330</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.835473</td>\n",
       "      <td>0.817352</td>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202500</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.250535</td>\n",
       "      <td>0.890915</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.914773</td>\n",
       "      <td>0.934236</td>\n",
       "      <td>0.780720</td>\n",
       "      <td>0.859977</td>\n",
       "      <td>0.818434</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.736508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.273825</td>\n",
       "      <td>0.889955</td>\n",
       "      <td>0.945841</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.935885</td>\n",
       "      <td>0.796840</td>\n",
       "      <td>0.823804</td>\n",
       "      <td>0.810098</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.764331</td>\n",
       "      <td>0.729483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207500</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.286469</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.928870</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.937368</td>\n",
       "      <td>0.822892</td>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.809721</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.728477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.264973</td>\n",
       "      <td>0.885797</td>\n",
       "      <td>0.957021</td>\n",
       "      <td>0.906723</td>\n",
       "      <td>0.931194</td>\n",
       "      <td>0.768828</td>\n",
       "      <td>0.857643</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.704142</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.730061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212500</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.266556</td>\n",
       "      <td>0.895713</td>\n",
       "      <td>0.950413</td>\n",
       "      <td>0.925663</td>\n",
       "      <td>0.937875</td>\n",
       "      <td>0.798687</td>\n",
       "      <td>0.851809</td>\n",
       "      <td>0.824393</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.737179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.303593</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.931648</td>\n",
       "      <td>0.942235</td>\n",
       "      <td>0.936911</td>\n",
       "      <td>0.819712</td>\n",
       "      <td>0.795799</td>\n",
       "      <td>0.807578</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = op.join(output_dir, 'checkpoint-15000-shiftLR')\n",
    "ckpt = False\n",
    "trainer.train(resume_from_checkpoint=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3bdf4-ef5b-46d1-ac2e-d97018072fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e89575-1f5f-4d15-8e61-dfd6cd2a8f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35b324-bcac-443f-93a5-8c01425f09a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b66ccc-f535-4331-8799-2b6dc396a29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
